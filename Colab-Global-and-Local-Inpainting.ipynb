{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Colab-Global-and-Local-Inpainting .ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sntGGdiqCjfS",
        "colab_type": "text"
      },
      "source": [
        "# Global-and-Local-Attention-Based-Free-Form-Image-Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3IbvBVPQ991",
        "colab_type": "text"
      },
      "source": [
        "[SayedNadim /Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting) combined with [mit-han-lab/data-efficient-gans ](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "Currently there are no pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpuDkZ1sPXep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EYZCvZPHL0i",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KraxPMANOycH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install miniconda and dependencies\n",
        "!git clone https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting\n",
        "%cd /content/\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install pytorch==1.1 cudatoolkit torchvision -c pytorch -y\n",
        "!conda install ipykernel -y\n",
        "#!pip install tensorboardX\n",
        "!pip install PyYAML\n",
        "!pip install OpenCV-python\n",
        "!pip install scipy==1.1\n",
        "!pip install tensorboardX==1.4\n",
        "!pip install tensorboard==1.11.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBE_6oHtO8kc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title config file\n",
        "%%writefile /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/configs/config.yaml \n",
        "# data parameters\n",
        "dataset_name: Test\n",
        "data_with_subfolder: False # apperantly this needs to be false, or no files will be detected\n",
        "train_data_path: /content/data\n",
        "resume: False\n",
        "checkpoint_dir: /content/checkpoints-training\n",
        "batch_size: 1\n",
        "image_shape: [256, 256, 3]\n",
        "mask_shape: [128, 128]\n",
        "mask_batch_same: True\n",
        "max_delta_shape: [32, 32]\n",
        "margin: [0, 0]\n",
        "discounted_mask: True\n",
        "spatial_discounting_gamma: 0.9\n",
        "random_crop: True\n",
        "mask_type: hole     # hole | mosaic\n",
        "mosaic_unit_size: 12\n",
        "save_image: 500\n",
        "\n",
        "# training parameters\n",
        "expname: v1\n",
        "cuda: True\n",
        "gpu_ids: [0]  # set the GPU ids to use, e.g. [0] or [1, 2]\n",
        "num_workers: 4\n",
        "lr: 0.0001\n",
        "beta1: 0.5\n",
        "beta2: 0.9\n",
        "niter: 1000000\n",
        "print_iter: 100\n",
        "viz_iter: 100\n",
        "viz_max_out: 16\n",
        "snapshot_save_iter: 500\n",
        "\n",
        "# loss weight\n",
        "coarse_l1_alpha: 1.2\n",
        "l1_loss_alpha: 1.2\n",
        "ae_loss_alpha: 1.2\n",
        "global_wgan_loss_alpha: 1.\n",
        "gan_loss_alpha: 0.001\n",
        "wgan_gp_lambda: 10\n",
        "\n",
        "# network parameters\n",
        "netG:\n",
        "  input_dim: 5\n",
        "  ngf: 32\n",
        "\n",
        "netD:\n",
        "  input_dim: 3\n",
        "  ndf: 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCA4b8rjN8W2",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modifying train.py with differentiable augmentation (experimental)\n",
        "%%writefile /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/scripts/trainer.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.network import Generator, GlobalDis\n",
        "from utils.logger import get_logger\n",
        "from torch.autograd import Variable\n",
        "from math import exp\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "\n",
        "class Trainer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Trainer, self).__init__()\n",
        "        self.config = config\n",
        "        self.use_cuda = self.config['cuda']\n",
        "        self.device_ids = self.config['gpu_ids']\n",
        "\n",
        "        self.netG = Generator(self.config['netG'], self.use_cuda, self.device_ids)\n",
        "        self.globalD = GlobalDis(self.config['netD'], self.use_cuda, self.device_ids)\n",
        "\n",
        "        self.optimizer_g = torch.optim.Adam(self.netG.parameters(), lr=self.config['lr'],\n",
        "                                            betas=(self.config['beta1'], self.config['beta2']))\n",
        "        d_params = list(self.globalD.parameters())\n",
        "        self.optimizer_d = torch.optim.Adam(d_params, lr=config['lr'],\n",
        "                                            betas=(self.config['beta1'], self.config['beta2']))\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.netG.to(self.device_ids[0])\n",
        "            self.globalD.to(self.device_ids[0])\n",
        "\n",
        "        self.ssim = SSIM()\n",
        "\n",
        "    def forward(self, x, masks, ground_truth):\n",
        "        self.train()\n",
        "        losses = {}\n",
        "\n",
        "        x1, x2 = self.netG(x, masks)\n",
        "        x1_inpaint = x1 * masks + x * (1. - masks)\n",
        "        x2_inpaint = x2 * masks + x * (1. - masks)\n",
        "\n",
        "        ## D part\n",
        "        refine_real, refine_fake = self.dis_forward(self.globalD, ground_truth, x2_inpaint.detach())\n",
        "        losses['d_loss_loren'] = torch.mean(torch.log(1.0 + torch.abs(refine_real - refine_fake)))\n",
        "        losses['d_loss_rel'] = (torch.mean(\n",
        "            torch.nn.ReLU()(1.0 - (refine_real - torch.mean(refine_fake)))) + torch.mean(\n",
        "            torch.nn.ReLU()(1.0 + (refine_fake - torch.mean(refine_real))))) / 2\n",
        "\n",
        "        ## G part\n",
        "        l1 = nn.L1Loss()(x1 * (1. - masks), ground_truth * (1. - masks)) * self.config['coarse_l1_alpha'] \\\n",
        "             + nn.L1Loss()(x2 * (1. - masks), ground_truth * (1. - masks))\n",
        "        ssim = ((1. - self.ssim(ground_truth, x1_inpaint)) + (1.0 - self.ssim(ground_truth, x2_inpaint))) / 2.0\n",
        "        losses['l1'] = l1 * 0.75 + ssim * 0.25\n",
        "\n",
        "        refine_real, refine_fake = self.dis_forward(self.globalD, ground_truth, x2_inpaint)\n",
        "        losses['g_loss_loren'] = torch.mean(torch.log(1.0 + torch.abs(refine_fake - refine_real)))\n",
        "        losses['g_loss_rel'] = (torch.mean(\n",
        "            torch.nn.ReLU()(1.0 + (refine_real - torch.mean(refine_fake)))) + torch.mean(\n",
        "            torch.nn.ReLU()(1.0 - (refine_fake - torch.mean(refine_real))))) / 2\n",
        "\n",
        "        return losses, x1_inpaint, x2_inpaint\n",
        "\n",
        "    def dis_forward(self, netD, ground_truth, x_inpaint):\n",
        "        assert ground_truth.size() == x_inpaint.size()\n",
        "        batch_size = ground_truth.size(0)\n",
        "        batch_data = torch.cat([ground_truth, x_inpaint], dim=0)\n",
        "        batch_output = netD(batch_data)\n",
        "        real_pred, fake_pred = torch.split(batch_output, batch_size, dim=0)\n",
        "        #real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
        "        real_pred = DiffAugment(real_pred, policy=policy)\n",
        "        fake_pred = DiffAugment(fake_pred, policy=policy)\n",
        "        return real_pred, fake_pred\n",
        "\n",
        "    def save_model(self, checkpoint_dir):\n",
        "        # Save generators, discriminators, and optimizers\n",
        "        gen_name = os.path.join(checkpoint_dir, 'gen.pt')\n",
        "        global_dis_name = os.path.join(checkpoint_dir, 'global_dis.pt')\n",
        "        gen_opt_name = os.path.join(checkpoint_dir, 'gen_optimizer.pt')\n",
        "        torch.save(self.netG.state_dict(), gen_name)\n",
        "        torch.save(self.globalD.state_dict(), global_dis_name)\n",
        "        torch.save(self.optimizer_g.state_dict(), gen_opt_name)\n",
        "\n",
        "    def resume(self, checkpoint_dir, iteration=1):\n",
        "        g_checkpoint = torch.load(f'{checkpoint_dir}/gen.pt')\n",
        "        global_dis_checkpoint = torch.load(f'{checkpoint_dir}/global_dis.pt')\n",
        "        self.netG.load_state_dict(g_checkpoint, strict=False)\n",
        "        self.globalD.load_state_dict(global_dis_checkpoint)\n",
        "        print(\"Model loaded\")\n",
        "        return iteration\n",
        "\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "\n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0azLgdbOpTo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Training\n",
        "%cd /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting\n",
        "!python train.py --config /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/configs/config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu_sKVx7FiBM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Show files\n",
        "%cd /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/checkpoints/Test/hole_v1\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUqayeCtKIxt",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title [Warning] Deleting Training Files\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrDBF9DWMDEt",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BJmENmPGwIm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Fixing test_single.py\n",
        "%%writefile /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/test_single.py\n",
        "def mask_image(x, config):\n",
        "    height, width, _ = config['image_shape']\n",
        "    max_mask = x.shape[0]\n",
        "    result = torch.ones_like(x)\n",
        "    mask = torch.ones(size=[x.shape[0], 1, x.shape[2], x.shape[3]])\n",
        "    for i in range(max_mask):\n",
        "        # mask_temp = random_mask(height=height, width=width)\n",
        "        mask_temp = brush_stroke_mask().generate_mask(height, width)\n",
        "        mask_temp_tensor = torch.tensor(mask_temp, dtype=torch.float32)\n",
        "        if x.is_cuda:\n",
        "            mask_temp_tensor.cuda()\n",
        "        result[i, :, :, :] = x[i, :, :, :] * (1. - mask_temp_tensor)\n",
        "        mask[i, :, :, :] = mask[i, :, :, :] * mask_temp_tensor\n",
        "    return result, mask\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from model.network import Generator\n",
        "from utils.tools import get_config, is_image_file, default_loader, normalize\n",
        "\n",
        "\n",
        "parser = ArgumentParser()\n",
        "parser.add_argument('--config', type=str, default='configs/test.yaml',\n",
        "                    help=\"training configuration\")\n",
        "parser.add_argument('--seed', type=int, default = '2019', help='manual seed')\n",
        "parser.add_argument('--image', type=str, default='example/image/image.jpg')\n",
        "parser.add_argument('--mask', type=str, default='example/mask/mask.png')\n",
        "parser.add_argument('--output', type=str, default='output/output.png')\n",
        "parser.add_argument('--flow', type=str, default='')\n",
        "parser.add_argument('--checkpoint_path', type=str, default='')\n",
        "parser.add_argument('--iter', type=int, default=0)\n",
        "\n",
        "def main():\n",
        "    args = parser.parse_args()\n",
        "    config = get_config(args.config)\n",
        "\n",
        "    # CUDA configuration\n",
        "    cuda = config['cuda']\n",
        "    device_ids = config['gpu_ids']\n",
        "    if cuda:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in device_ids)\n",
        "        device_ids = list(range(len(device_ids)))\n",
        "        config['gpu_ids'] = device_ids\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    print(\"Arguments: {}\".format(args))\n",
        "\n",
        "    # Set random seed\n",
        "    if args.seed is None:\n",
        "        args.seed = random.randint(1, 10000)\n",
        "    print(\"Random seed: {}\".format(args.seed))\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    print(\"Configuration: {}\".format(config))\n",
        "\n",
        "    try:  # for unexpected error logging\n",
        "        with torch.no_grad():   # enter no grad context\n",
        "            if is_image_file(args.image):\n",
        "                if args.mask and is_image_file(args.mask):\n",
        "                    # Test a single masked image with a given mask\n",
        "                    x = default_loader(args.image)\n",
        "                    mask = default_loader(args.mask)\n",
        "                    x = transforms.Resize(config['image_shape'][:-1])(x)\n",
        "                    # x = transforms.CenterCrop(config['image_shape'][:-1])(x)\n",
        "                    mask = transforms.Resize(config['image_shape'][:-1])(mask)\n",
        "                    # mask = transforms.CenterCrop(config['image_shape'][:-1])(mask)\n",
        "                    x = transforms.ToTensor()(x)\n",
        "                    mask = transforms.ToTensor()(mask)[0].unsqueeze(dim=0)\n",
        "                    x = normalize(x)\n",
        "                    x = x * (1. - mask)\n",
        "                    x = x.unsqueeze(dim=0)\n",
        "                    x_raw = x\n",
        "                    mask = mask.unsqueeze(dim=0)\n",
        "                elif args.mask:\n",
        "                    raise TypeError(\"{} is not an image file.\".format(args.mask))\n",
        "                else:\n",
        "                    # Test a single ground-truth image with a random mask\n",
        "                    ground_truth = default_loader(args.image)\n",
        "                    ground_truth = transforms.ToTensor()(ground_truth)\n",
        "                    ground_truth = normalize(ground_truth)\n",
        "                    ground_truth = ground_truth.unsqueeze(dim=0)\n",
        "                    bboxes = test_bbox(config, batch_size=ground_truth.size(0), t = 50, l = 60)\n",
        "                    x, mask = mask_image(ground_truth, bboxes, config)\n",
        "\n",
        "                # Set checkpoint path\n",
        "                if not args.checkpoint_path:\n",
        "                    checkpoint_path = os.path.join('checkpoints',\n",
        "                                                   config['dataset_name'],\n",
        "                                                   config['mask_type'] + '_' + config['expname'])\n",
        "                else:\n",
        "                    checkpoint_path = args.checkpoint_path\n",
        "\n",
        "                # Define the trainer\n",
        "                netG = Generator(config['netG'], cuda, device_ids)\n",
        "                # Resume weight\n",
        "                g_checkpoint = torch.load(f'{checkpoint_path}/gen.pt')\n",
        "                netG.load_state_dict(g_checkpoint, strict=False)\n",
        "                # model_iteration = int(last_model_name[-11:-3])\n",
        "                print(\"Model Resumed\".format(checkpoint_path))\n",
        "\n",
        "                if cuda:\n",
        "                    netG = nn.parallel.DataParallel(netG, device_ids=device_ids)\n",
        "                    x = x.cuda()\n",
        "                    mask = mask.cuda()\n",
        "\n",
        "                # Inference\n",
        "                x1, x2 = netG(x, mask)\n",
        "                inpainted_result = x2 * mask + x * (1. - mask)\n",
        "\n",
        "                vutils.save_image(inpainted_result, args.output, padding=0, normalize=True)\n",
        "                print(\"Saved the inpainted result to {}\".format(args.output))\n",
        "            else:\n",
        "                raise TypeError(\"{} is not an image file.\".format)\n",
        "        # exit no grad context\n",
        "    except Exception as e:  # for unexpected error logging\n",
        "        print(\"Error: {}\".format(e))\n",
        "        raise e\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZs8LxngWhMI",
        "colab_type": "text"
      },
      "source": [
        "The output size can be configured by changing the image_size in the yaml file. \n",
        "- K80 11441MiB: 600x600\n",
        "- P100 16GB: ~1000x1000?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv4WWXyuF0Cn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test model\n",
        "%cd /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting\n",
        "!python test_single.py --config /content/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting/configs/config.yaml \\\n",
        "--image /content/image.png --mask /content/mask.png --output /content/output.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4moSxJDVst7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
